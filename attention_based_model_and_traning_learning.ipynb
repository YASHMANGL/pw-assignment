{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269d9d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 How to implement a simple text classification model using LSTM in Keras \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "import keras\n",
    "\n",
    "texts = [...]   # A list of raw text samples\n",
    "labels = [...]  # Corresponding integer labels\n",
    "\n",
    "max_words = 10000\n",
    "max_len = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "x = pad_sequences(sequences, maxlen=max_len)\n",
    "y = keras.utils.to_categorical(labels)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 100, input_length=max_len))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x, y, batch_size=32, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e9c15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 How to generate sequences of text using a Recurrent Neural Network (RNN) \n",
    "# After training an RNN model (e.g., LSTM or GRU), use:\n",
    "def generate_text(model, seed_text, length):\n",
    "    for _ in range(length):\n",
    "        tokenized_input = ... # preprocess as required\n",
    "        preds = model.predict(tokenized_input)[0]\n",
    "        next_token = ... # sample or argmax\n",
    "        seed_text += next_token\n",
    "    return seed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36148d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3How to perform sentiment analysis using a simple CNN model \n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "model = Sequential([\n",
    "    Embedding(max_words, 128, input_length=max_len),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x, y, epochs=5, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b44730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 How to perform Named Entity Recognition (NER) using spaCy \n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 How to implement a simple Seq2Seq model for machine translation using LSTM in Keras \n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acca1c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 How to generate text using a pre-trained transformer model (GPT-2) \n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_ids = tokenizer.encode(\"The future of AI is\", return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "print(tokenizer.decode(output, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516abe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7How to apply data augmentation for text in NLP \n",
    "\n",
    "# Common methods:\n",
    "# Synonym replacement (using WordNet or embedding neighbors)\n",
    "# Random deletion, random swap, or random insertion\n",
    "# Back translation (translate to another language, then back)\n",
    "# Contextual augmentation (masked language model predictions)\n",
    "# Character-level perturbations\n",
    "# Libraries: NLPAug, TextAttack, or nlpaug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa6039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8How can you add an Attention Mechanism to a Seq2Seq model?\n",
    "\n",
    "from keras.layers import Attention\n",
    "\n",
    "# encoder_outputs: (batch, input_seq_len, units)\n",
    "# decoder_outputs: (batch, target_seq_len, units)\n",
    "attention_layer = Attention()\n",
    "context_vector = attention_layer([decoder_outputs, encoder_outputs])\n",
    "# Concatenate context with decoder for final outputs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
